{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NJq_jEvmHVUs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "from random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "crgibHsiHVUv"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a[0]</th>\n",
              "      <th>a[1]</th>\n",
              "      <th>a[2]</th>\n",
              "      <th>a[3]</th>\n",
              "      <th>eigen_values</th>\n",
              "      <th>results</th>\n",
              "      <th>numeric_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>-2</td>\n",
              "      <td>2</td>\n",
              "      <td>-3</td>\n",
              "      <td>['8.65685424949238', '-2.65685424949238']</td>\n",
              "      <td>1</td>\n",
              "      <td>([ 1.00000000e+00, -1.20000000e+01,  9.9977247...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-9</td>\n",
              "      <td>-3</td>\n",
              "      <td>2</td>\n",
              "      <td>-4</td>\n",
              "      <td>['-7.0', '-6.0']</td>\n",
              "      <td>0</td>\n",
              "      <td>([ 1.00000000e+00, -1.40000000e+01,  9.9971614...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-5</td>\n",
              "      <td>-9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>['(-1.9999999999999998+3.000000000000001j)', '...</td>\n",
              "      <td>0</td>\n",
              "      <td>([ 1.00000000e+00, -1.40000000e+01,  9.9973320...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-7</td>\n",
              "      <td>-7</td>\n",
              "      <td>-1</td>\n",
              "      <td>-5</td>\n",
              "      <td>['-8.82842712474619', '-3.17157287525381']</td>\n",
              "      <td>0</td>\n",
              "      <td>([0], [0])</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>-7</td>\n",
              "      <td>-9</td>\n",
              "      <td>4</td>\n",
              "      <td>['13.577747210701755', '-2.577747210701756']</td>\n",
              "      <td>1</td>\n",
              "      <td>([   0,    2,    4,    6,    8,   10,   12,   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>2</td>\n",
              "      <td>-6</td>\n",
              "      <td>8</td>\n",
              "      <td>-9</td>\n",
              "      <td>['(-3.500000000000001+4.21307488658818j)', '(-...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>8</td>\n",
              "      <td>-9</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>['(6+5.65685424949238j)', '(6-5.65685424949238...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-3</td>\n",
              "      <td>-9</td>\n",
              "      <td>8</td>\n",
              "      <td>-9</td>\n",
              "      <td>['(-6+7.937253933193773j)', '(-6-7.93725393319...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-6</td>\n",
              "      <td>-1</td>\n",
              "      <td>4</td>\n",
              "      <td>-7</td>\n",
              "      <td>['(-6.5+1.9364916731037083j)', '(-6.5-1.936491...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>3</td>\n",
              "      <td>-3</td>\n",
              "      <td>7</td>\n",
              "      <td>-9</td>\n",
              "      <td>['0.8729833462074161', '-6.872983346207417']</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1001 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      a[0]  a[1]  a[2]  a[3]  \\\n",
              "0        9    -2     2    -3   \n",
              "1       -9    -3     2    -4   \n",
              "2       -5    -9     2     1   \n",
              "3       -7    -7    -1    -5   \n",
              "4        7    -7    -9     4   \n",
              "...    ...   ...   ...   ...   \n",
              "996      2    -6     8    -9   \n",
              "997      8    -9     4     4   \n",
              "998     -3    -9     8    -9   \n",
              "999     -6    -1     4    -7   \n",
              "1000     3    -3     7    -9   \n",
              "\n",
              "                                           eigen_values  results  \\\n",
              "0             ['8.65685424949238', '-2.65685424949238']        1   \n",
              "1                                      ['-7.0', '-6.0']        0   \n",
              "2     ['(-1.9999999999999998+3.000000000000001j)', '...        0   \n",
              "3            ['-8.82842712474619', '-3.17157287525381']        0   \n",
              "4          ['13.577747210701755', '-2.577747210701756']        1   \n",
              "...                                                 ...      ...   \n",
              "996   ['(-3.500000000000001+4.21307488658818j)', '(-...        0   \n",
              "997   ['(6+5.65685424949238j)', '(6-5.65685424949238...        1   \n",
              "998   ['(-6+7.937253933193773j)', '(-6-7.93725393319...        0   \n",
              "999   ['(-6.5+1.9364916731037083j)', '(-6.5-1.936491...        0   \n",
              "1000       ['0.8729833462074161', '-6.872983346207417']        1   \n",
              "\n",
              "                                         numeric_output  \n",
              "0     ([ 1.00000000e+00, -1.20000000e+01,  9.9977247...  \n",
              "1     ([ 1.00000000e+00, -1.40000000e+01,  9.9971614...  \n",
              "2     ([ 1.00000000e+00, -1.40000000e+01,  9.9973320...  \n",
              "3                                            ([0], [0])  \n",
              "4     ([   0,    2,    4,    6,    8,   10,   12,   ...  \n",
              "...                                                 ...  \n",
              "996                                                 NaN  \n",
              "997                                                 NaN  \n",
              "998                                                 NaN  \n",
              "999                                                 NaN  \n",
              "1000                                                NaN  \n",
              "\n",
              "[1001 rows x 7 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"./out/demo_labelled_dataset.csv\")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8S3veKWwHVUw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      ([ 1.00000000e+00, -1.20000000e+01,  9.9977247...\n",
              "1      ([ 1.00000000e+00, -1.40000000e+01,  9.9971614...\n",
              "2      ([ 1.00000000e+00, -1.40000000e+01,  9.9973320...\n",
              "3                                             ([0], [0])\n",
              "4      ([   0,    2,    4,    6,    8,   10,   12,   ...\n",
              "                             ...                        \n",
              "921    ([ 1.00000000e+00, -1.70000000e+01,  9.9971169...\n",
              "922    ([1.00000000e+00, 6.00000000e+00, 1.00015154e+...\n",
              "923                                           ([0], [0])\n",
              "924    ([   0,    2,    4,    6,    8,   10,   12,   ...\n",
              "925                                           ([0], [0])\n",
              "Name: numeric_output, Length: 926, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = df[\"numeric_output\"][:926]\n",
        "y = df.results[:926]\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xxBeuxHmHVUx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "X = X.apply(ast.literal_eval)\n",
        "\n",
        "print(type(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A8w7rwRVHVUx"
      },
      "outputs": [],
      "source": [
        "# train_X, val_X, train_y, val_y = train_test_split(X, y)\n",
        "\n",
        "# train_X, val_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA_XkGUFHVUx",
        "outputId": "284bada9-00c9-4189-a2a1-44b842d0d721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0     [[1.0, -12.0, 0.999772471, -11.9978384, 0.9995...\n",
            "1     [[1.0, -14.0, 0.999716142, -13.9991281, 0.9994...\n",
            "2     [[1.0, -14.0, 0.999733204, -13.997332, 0.99946...\n",
            "3                                            [[0], [0]]\n",
            "4     [[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 2...\n",
            "                            ...                        \n",
            "95                                           [[0], [0]]\n",
            "96    [[1.0, -11.0, 0.99975506, -11.0005118, 0.99951...\n",
            "97                                           [[0], [0]]\n",
            "98                                           [[0], [0]]\n",
            "99    [[1.0, 1.0, 1.000041, 0.998727523, 1.00008195,...\n",
            "Name: numeric_output, Length: 100, dtype: object\n"
          ]
        }
      ],
      "source": [
        "numeric_series = X[:100]\n",
        "\n",
        "# print(numeric_series)\n",
        "\n",
        "# numeric_series = pd.Series([\n",
        "#     ([1.0], [-12.0, 0.999772471]),\n",
        "#     ([-11.9978384],[ 0.999544983]),\n",
        "#     ([0.99909009], [-11.9913555, 0.99863536]),\n",
        "#     ([-11.9870352],[ 0.998180794]),\n",
        "#     ([0.993644133], [-11.9396045, 0.989123796]),\n",
        "#     ([-11.9913555, 0.99863536], [-12.0, 0.999772471]),\n",
        "#     ([-11.9396045],[ 0.999544983]),\n",
        "#     ([0.99909009], [-11.9870352]),\n",
        "#     ([-11.9870352],[ 0.998180794]),\n",
        "#     ([0.993644133], [-11.9396045, 0.989123796])\n",
        "# ])\n",
        "\n",
        "# print(numeric_series)\n",
        "\n",
        "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "vocab_size = len(word_dict)\n",
        "id = len(word_dict)\n",
        "\n",
        "for lst1, lst2 in numeric_series:\n",
        "    for i, num in enumerate(lst1):\n",
        "        token = str(num)\n",
        "        if token not in word_dict:\n",
        "            word_dict[token] = id\n",
        "            id += 1\n",
        "    for i, num in enumerate(lst2):\n",
        "        token = str(num)\n",
        "        if token not in word_dict:\n",
        "            word_dict[token] = id\n",
        "            id += 1\n",
        "\n",
        "number_dict = {i: w for w, i in word_dict.items()}\n",
        "\n",
        "tokenized_series = numeric_series.apply(lambda lst: [word_dict.get(str(num), num) for num in lst])\n",
        "\n",
        "print(tokenized_series)\n",
        "\n",
        "token_list = list()\n",
        "for xsol, ysol in tokenized_series:\n",
        "    x_tokens = []\n",
        "    y_tokens = []\n",
        "    for num in xsol:\n",
        "        x_tokens.append(word_dict[str(num)])\n",
        "    for num in ysol:\n",
        "        y_tokens.append(word_dict[str(num)])\n",
        "    tokens = [x_tokens, y_tokens]\n",
        "    token_list.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "max_length = max(len(lst) for tup in tokenized_series for lst in tup)\n",
        "print(max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXkSj4wGHVUy",
        "outputId": "9ef1a29d-e84f-49e1-b875-cbe7ebd83ca8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[CLS]': 1,\n",
              " '[SEP]': 2,\n",
              " '[MASK]': 3,\n",
              " '1.0': 4,\n",
              " '-12.0': 5,\n",
              " '0.999772471': 6,\n",
              " '-11.9978384': 7,\n",
              " '0.999544983': 8,\n",
              " '-11.9956772': 9,\n",
              " '0.99909009': 10,\n",
              " '-11.9913555': 11,\n",
              " '0.99863536': 12,\n",
              " '-11.9870352': 13,\n",
              " '0.998180794': 14,\n",
              " '-11.9827163': 15,\n",
              " '0.993644133': 16,\n",
              " '-11.9396045': 17,\n",
              " '0.989123796': 18,\n",
              " '-11.8966327': 19,\n",
              " '0.984619732': 20,\n",
              " '-11.8538006': 21,\n",
              " '0.980131887': 22,\n",
              " '-11.8111077': 23,\n",
              " '0.936133967': 24,\n",
              " '-11.3917432': 25,\n",
              " '0.893700961': 26,\n",
              " '-10.985839': 27,\n",
              " '0.852782605': 28,\n",
              " '-10.592987': 29,\n",
              " '0.813330146': 30,\n",
              " '-10.2127909': 31,\n",
              " '0.775296315': 32,\n",
              " '-9.84486579': 33,\n",
              " '0.688508144': 34,\n",
              " '-8.99968176': 35,\n",
              " '0.609205494': 36,\n",
              " '-8.21968239': 37,\n",
              " '0.550677138': 38,\n",
              " '-7.63856775': 39,\n",
              " '0.496303774': 40,\n",
              " '-7.09396696': 41,\n",
              " '0.445823782': 42,\n",
              " '-6.58372687': 43,\n",
              " '0.398991006': 44,\n",
              " '-6.10581629': 45,\n",
              " '0.355573856': 46,\n",
              " '-5.65831946': 47,\n",
              " '0.315354481': 48,\n",
              " '-5.23942953': 49,\n",
              " '0.243701736': 50,\n",
              " '-4.48075215': 51,\n",
              " '0.182536067': 52,\n",
              " '-3.81728698': 53,\n",
              " '0.139852838': 54,\n",
              " '-3.34298208': 55,\n",
              " '0.102529912': 56,\n",
              " '-2.91833089': 57,\n",
              " '0.0700036155': 58,\n",
              " '-2.53854891': 59,\n",
              " '0.0417646994': 60,\n",
              " '-2.19929101': 61,\n",
              " '0.017353365': 62,\n",
              " '-1.89661197': 63,\n",
              " '-0.00364530275': 64,\n",
              " '-1.62693105': 65,\n",
              " '-0.0216054888': 66,\n",
              " '-1.38699935': 67,\n",
              " '-0.0417587807': 68,\n",
              " '-1.10313947': 69,\n",
              " '-0.0576607006': 70,\n",
              " '-0.862293373': 71,\n",
              " '-0.069961648': 72,\n",
              " '-0.658677961': 73,\n",
              " '-0.0792241703': 74,\n",
              " '-0.487239267': 75,\n",
              " '-0.0859340701': 76,\n",
              " '-0.343564166': 77,\n",
              " '-0.0905101645': 78,\n",
              " '-0.223802413': 79,\n",
              " '-0.0933128543': 80,\n",
              " '-0.124597939': 81,\n",
              " '-0.0947823911': 82,\n",
              " '-0.0262931193': 83,\n",
              " '-0.0945129281': 84,\n",
              " '0.0498781502': 85,\n",
              " '-0.0929068822': 86,\n",
              " '0.107859921': 87,\n",
              " '-0.0902942384': 88,\n",
              " '0.150961877': 89,\n",
              " '-0.0869442681': 90,\n",
              " '0.18195533': 91,\n",
              " '-0.0830754629': 92,\n",
              " '0.203155878': 93,\n",
              " '-0.0788639302': 94,\n",
              " '0.216493502': 95,\n",
              " '-0.0744505022': 96,\n",
              " '0.22357233': 97,\n",
              " '-0.0688351574': 98,\n",
              " '0.22562514': 99,\n",
              " '-0.0632421647': 100,\n",
              " '0.222085828': 101,\n",
              " '-0.0577898643': 102,\n",
              " '0.214581789': 103,\n",
              " '-0.0525606939': 104,\n",
              " '0.204377995': 105,\n",
              " '-0.0476092947': 106,\n",
              " '0.192449261': 107,\n",
              " '-0.0429689911': 108,\n",
              " '0.179538852': 109,\n",
              " '-0.0386569537': 110,\n",
              " '0.166206068': 111,\n",
              " '-0.0346782885': 112,\n",
              " '0.152864739': 113,\n",
              " '-0.0309886572': 114,\n",
              " '0.13966484': 115,\n",
              " '-0.027625964': 116,\n",
              " '0.126979494': 117,\n",
              " '-0.024575268': 118,\n",
              " '0.114953342': 119,\n",
              " '-0.02181871': 120,\n",
              " '0.103675582': 121,\n",
              " '-0.019336735': 122,\n",
              " '0.0931934498': 123,\n",
              " '-0.0171090106': 124,\n",
              " '0.0835228716': 125,\n",
              " '-0.0151151073': 126,\n",
              " '0.074656862': 127,\n",
              " '-0.0133349904': 128,\n",
              " '0.0665720888': 129,\n",
              " '-0.0114822502': 130,\n",
              " '0.0579849177': 131,\n",
              " '-0.00987067146': 132,\n",
              " '0.0503686542': 133,\n",
              " '-0.00847246788': 134,\n",
              " '0.0436462033': 135,\n",
              " '-0.00726220044': 136,\n",
              " '0.0377376509': 137,\n",
              " '-0.00621680981': 138,\n",
              " '0.032563677': 139,\n",
              " '-0.0053155613': 140,\n",
              " '0.02804782': 141,\n",
              " '-0.00453993715': 142,\n",
              " '0.0241178846': 143,\n",
              " '-0.00387349554': 144,\n",
              " '0.0207068084': 145,\n",
              " '-0.00330171016': 146,\n",
              " '0.0177530773': 147,\n",
              " '-0.00281180292': 148,\n",
              " '0.0152008372': 149,\n",
              " '-0.00233818572': 150,\n",
              " '0.0127130219': 151,\n",
              " '-0.00194236427': 152,\n",
              " '0.0106169851': 153,\n",
              " '-0.00161202021': 154,\n",
              " '0.00885469729': 155,\n",
              " '-0.00133667596': 156,\n",
              " '0.0073758457': 157,\n",
              " '-0.00110744615': 158,\n",
              " '0.00613698154': 159,\n",
              " '-0.000916817513': 160,\n",
              " '0.00510079359': 161,\n",
              " '-0.000737549353': 162,\n",
              " '0.00412082045': 163,\n",
              " '-0.000592807032': 164,\n",
              " '0.00332514468': 165,\n",
              " '-0.000476075655': 166,\n",
              " '0.00268014164': 167,\n",
              " '-0.000382034641': 168,\n",
              " '0.00215804028': 169,\n",
              " '-0.000306348393': 170,\n",
              " '0.00173599339': 171,\n",
              " '-0.000245490363': 172,\n",
              " '0.00139525089': 173,\n",
              " '-0.00018372963': 174,\n",
              " '0.00104797474': 175,\n",
              " '-0.000137367311': 176,\n",
              " '0.000786111193': 177,\n",
              " '-0.00010261057': 178,\n",
              " '0.000588981314': 179,\n",
              " '-7.65834344e-05': 180,\n",
              " '0.000440812273': 181,\n",
              " '-5.71131534e-05': 182,\n",
              " '0.000329592239': 183,\n",
              " '-4.25617728e-05': 184,\n",
              " '0.000246211129': 185,\n",
              " '-3.16962266e-05': 186,\n",
              " '0.000183755368': 187,\n",
              " '-2.55731162e-05': 188,\n",
              " '0.00014848564': 189,\n",
              " '-2.06260745e-05': 190,\n",
              " '0.000119934868': 191,\n",
              " '-1.66311523e-05': 192,\n",
              " '9.68355594e-05': 193,\n",
              " '-1.34061551e-05': 194,\n",
              " '7.81602883e-05': 195,\n",
              " '-1.08035276e-05': 196,\n",
              " '6.30637924e-05': 197,\n",
              " '-8.70395128e-06': 198,\n",
              " '5.08670377e-05': 199,\n",
              " '-6.59789322e-06': 200,\n",
              " '3.86143097e-05': 201,\n",
              " '-4.99926065e-06': 202,\n",
              " '2.92962689e-05': 203,\n",
              " '-3.78666951e-06': 204,\n",
              " '2.22161497e-05': 205,\n",
              " '-2.86728088e-06': 206,\n",
              " '1.68418341e-05': 207,\n",
              " '-2.17041227e-06': 208,\n",
              " '1.27626923e-05': 209,\n",
              " '-1.40900017e-06': 210,\n",
              " '8.29242538e-06': 211,\n",
              " '-9.14689218e-07': 212,\n",
              " '5.38214217e-06': 213,\n",
              " '-5.93721716e-07': 214,\n",
              " '3.49864711e-06': 215,\n",
              " '-3.85028183e-07': 216,\n",
              " '2.27189563e-06': 217,\n",
              " '-2.49621498e-07': 218,\n",
              " '1.47357524e-06': 219,\n",
              " '-1.43239347e-07': 220,\n",
              " '8.4535558e-07': 221,\n",
              " '-8.2172892e-08': 222,\n",
              " '4.95050053e-07': 223,\n",
              " '-4.6453272e-08': 224,\n",
              " '2.853316e-07': 225,\n",
              " '-2.61258952e-08': 226,\n",
              " '1.59399314e-07': 227,\n",
              " '-1.4290292e-08': 228,\n",
              " '8.38967471e-08': 229,\n",
              " '-8.06944009e-09': 230,\n",
              " '4.39808728e-08': 231,\n",
              " '-4.69152989e-09': 232,\n",
              " '2.545642e-08': 233,\n",
              " '-2.7203905e-09': 234,\n",
              " '1.52441373e-08': 235,\n",
              " '-1.55478316e-09': 236,\n",
              " '8.82360153e-09': 237,\n",
              " '-8.85955333e-10': 238,\n",
              " '4.9865154e-09': 239,\n",
              " '-5.09416993e-10': 240,\n",
              " '2.85141336e-09': 241,\n",
              " '-2.93388679e-10': 242,\n",
              " '1.64537848e-09': 243,\n",
              " '2.10503743e-10': 244,\n",
              " '-1.25676393e-09': 245,\n",
              " '1.16486667e-10': 246,\n",
              " '-6.95785837e-10': 247,\n",
              " '2.61921874e-11': 248,\n",
              " '-1.56577521e-10': 249,\n",
              " '2.02068727e-12': 250,\n",
              " '-1.20857501e-11': 251,\n",
              " '1.55831057e-13': 252,\n",
              " '-9.32428106e-13': 253,\n",
              " '1.20132427e-14': 254,\n",
              " '-7.19089072e-14': 255,\n",
              " '9.25844816e-16': 256,\n",
              " '-5.54369895e-15': 257,\n",
              " '7.13354467e-17': 258,\n",
              " '-4.27254684e-16': 259,\n",
              " '5.49511638e-18': 260,\n",
              " '-3.29201652e-17': 261,\n",
              " '6.79889668e-19': 262,\n",
              " '-4.073889e-18': 263,\n",
              " '8.41074883e-20': 264,\n",
              " '-5.04057361e-19': 265,\n",
              " '-2.0': 266,\n",
              " '0.999962066': 267,\n",
              " '-2.00030332': 268,\n",
              " '0.999924126': 269,\n",
              " '-2.00060654': 270,\n",
              " '0.999848235': 271,\n",
              " '-2.00121276': 272,\n",
              " '0.999772321': 273,\n",
              " '-2.00181856': 274,\n",
              " '0.999696384': 275,\n",
              " '-2.00242395': 276,\n",
              " '0.998935753': 277,\n",
              " '-2.00845475': 278,\n",
              " '0.998172843': 279,\n",
              " '-2.01444378': 280,\n",
              " '0.99740767': 281,\n",
              " '-2.02039121': 282,\n",
              " '0.996640248': 283,\n",
              " '-2.02629722': 284,\n",
              " '0.988845816': 285,\n",
              " '-2.08311533': 286,\n",
              " '0.98084345': 287,\n",
              " '-2.13597188': 288,\n",
              " '0.972647866': 289,\n",
              " '-2.18502626': 290,\n",
              " '0.964273194': 291,\n",
              " '-2.23043248': 292,\n",
              " '0.955732985': 293,\n",
              " '-2.27233931': 294,\n",
              " '0.934369503': 295,\n",
              " '-2.36046175': 296,\n",
              " '0.912277658': 297,\n",
              " '-2.43069966': 298,\n",
              " '0.894157825': 299,\n",
              " '-2.475277': 300,\n",
              " '0.875744334': 301,\n",
              " '-2.5103698': 302,\n",
              " '0.857104274': 303,\n",
              " '-2.53676953': 304,\n",
              " '0.838299078': 305,\n",
              " '-2.5552143': 306,\n",
              " '0.819384919': 307,\n",
              " '-2.56639196': 308,\n",
              " '0.800413066': 309,\n",
              " '-2.5709433': 310,\n",
              " '0.762478826': 311,\n",
              " '-2.56251181': 312,\n",
              " '0.724820759': 313,\n",
              " '-2.53421085': 314,\n",
              " '0.694768866': 315,\n",
              " '-2.49936982': 316,\n",
              " '0.665187223': 317,\n",
              " '-2.45568906': 318,\n",
              " '0.636172128': 319,\n",
              " '-2.40468129': 320,\n",
              " '0.607802905': 321,\n",
              " '-2.34768222': 322,\n",
              " '0.580143896': 323,\n",
              " '-2.28586886': 324,\n",
              " '0.553246259': 325,\n",
              " '-2.22027565': 326,\n",
              " '0.527149583': 327,\n",
              " '-2.15180931': 328,\n",
              " '0.492989504': 329,\n",
              " '-2.05547558': 330,\n",
              " '0.460413236': 331,\n",
              " '-1.95697436': 332,\n",
              " '0.429444272': 333,\n",
              " '-1.8577004': 334,\n",
              " '0.400085607': 335,\n",
              " '-1.75879078': 336,\n",
              " '0.372323606': 337,\n",
              " '-1.66116257': 338,\n",
              " '0.346131304': 339,\n",
              " '-1.56554555': 340,\n",
              " '0.321471214': 341,\n",
              " '-1.47251057': 342,\n",
              " '0.293111557': 343,\n",
              " '-1.36201103': 344,\n",
              " '0.266912744': 345,\n",
              " '-1.25667685': 346,\n",
              " '0.242767323': 347,\n",
              " '-1.15688303': 348,\n",
              " '0.220562078': 349,\n",
              " '-1.06283677': 350,\n",
              " '0.200181023': 351,\n",
              " '-0.974612622': 352,\n",
              " '0.18150776': 353,\n",
              " '-0.892181961': 354,\n",
              " '0.164427289': 355,\n",
              " '-0.815437016': 356,\n",
              " '0.148827395': 357,\n",
              " '-0.744210581': 358,\n",
              " '0.131297089': 359,\n",
              " '-0.662858665': 360,\n",
              " '0.115697876': 361,\n",
              " '-0.589275822': 362,\n",
              " '0.101842324': 363,\n",
              " '-0.522949014': 364,\n",
              " '0.0895560777': 365,\n",
              " '-0.46334569': 366,\n",
              " '0.0786781256': 367,\n",
              " '-0.409931084': 368,\n",
              " '0.0690606859': 369,\n",
              " '-0.362180721': 370,\n",
              " '0.0605688349': 371,\n",
              " '-0.319589242': 372,\n",
              " '0.0530799502': 373,\n",
              " '-0.281676374': 374,\n",
              " '0.0464110225': 375,\n",
              " '-0.247621401': 376,\n",
              " '0.0405513919': 377,\n",
              " '-0.217457492': 378,\n",
              " '0.0354080218': 379,\n",
              " '-0.190782621': 380,\n",
              " '0.0308976014': 381,\n",
              " '-0.167227824': 382,\n",
              " '0.0269457206': 383,\n",
              " '-0.146456351': 384,\n",
              " '0.0234860736': 385,\n",
              " '-0.128162314': 386,\n",
              " '0.0204597005': 387,\n",
              " '-0.112069016': 388,\n",
              " '0.0178142744': 389,\n",
              " '-0.0979270777': 390,\n",
              " '0.0151184435': 391,\n",
              " '-0.0834382727': 392,\n",
              " '0.0128224601': 393,\n",
              " '-0.0710311793': 394,\n",
              " '0.01086867': 395,\n",
              " '-0.0604196156': 396,\n",
              " '0.00920738875': 397,\n",
              " '-0.05135396': 398,\n",
              " '0.00779587434': 399,\n",
              " '-0.0436171204': 400,\n",
              " '0.00659741396': 401,\n",
              " '-0.0370207814': 402,\n",
              " '0.00558051926': 403,\n",
              " '-0.0314019534': 404,\n",
              " '0.00471821905': 405,\n",
              " '-0.0266198685': 406,\n",
              " '0.00398743963': 407,\n",
              " '-0.0225531798': 408,\n",
              " '0.00336846426': 409,\n",
              " '-0.0190974634': 410,\n",
              " '0.002776885': 411,\n",
              " '-0.0157839121': 412,\n",
              " '0.00228810021': 413,\n",
              " '-0.0130371321': 414,\n",
              " '0.00188449276': 415,\n",
              " '-0.0107620172': 416,\n",
              " '0.0015514096': 417,\n",
              " '-0.00887899694': 418,\n",
              " '0.00127667622': 419,\n",
              " '-0.00732160392': 420,\n",
              " '0.00105018652': 421,\n",
              " '-0.00603438687': 422,\n",
              " '0.000839039717': 423,\n",
              " '-0.00483126723': 424,\n",
              " '0.000670038063': 425,\n",
              " '-0.00386578022': 426,\n",
              " '0.000534846055': 427,\n",
              " '-0.00309154621': 428,\n",
              " '0.000426757177': 429,\n",
              " '-0.00247110761': 430,\n",
              " '0.00034038083': 431,\n",
              " '-0.00197422818': 432,\n",
              " '0.000271388002': 433,\n",
              " '-0.00157653964': 434,\n",
              " '0.000201865399': 435,\n",
              " '-0.00117493719': 436,\n",
              " '0.00015006568': 437,\n",
              " '-0.000875003121': 438,\n",
              " '0.000111503211': 439,\n",
              " '-0.000651201991': 440,\n",
              " '8.2811722e-05': 441,\n",
              " '-0.000484402187': 442,\n",
              " '6.14742732e-05': 443,\n",
              " '-0.000360119055': 444,\n",
              " '4.5616225e-05': 445,\n",
              " '-0.000267583843': 446,\n",
              " '3.38352885e-05': 447,\n",
              " '-0.000198732466': 448,\n",
              " '2.72239482e-05': 449,\n",
              " '-0.000160041144': 450,\n",
              " '2.19003347e-05': 451,\n",
              " '-0.000128853895': 452,\n",
              " '1.76146507e-05': 453,\n",
              " '-0.000103720664': 454,\n",
              " '1.41652399e-05': 455,\n",
              " '-8.34737425e-05': 456,\n",
              " '1.13894323e-05': 457,\n",
              " '-6.71651677e-05': 458,\n",
              " '9.15616441e-06': 459,\n",
              " '-5.40326917e-05': 460,\n",
              " '6.92247484e-06': 461,\n",
              " '-4.08863336e-05': 462,\n",
              " '5.23221825e-06': 463,\n",
              " '-3.0926723e-05': 464,\n",
              " '3.9538747e-06': 465,\n",
              " '-2.33861351e-05': 466,\n",
              " '2.98729957e-06': 467,\n",
              " '-1.76816097e-05': 468,\n",
              " '2.25655937e-06': 469,\n",
              " '-1.33654783e-05': 470,\n",
              " '1.46051084e-06': 471,\n",
              " '-8.65180668e-06': 472,\n",
              " '9.45583957e-07': 473,\n",
              " '-5.59616603e-06': 474,\n",
              " '6.12296162e-07': 475,\n",
              " '-3.62753177e-06': 476,\n",
              " '3.96163017e-07': 477,\n",
              " '-2.34932138e-06': 478,\n",
              " '2.56306344e-07': 479,\n",
              " '-1.51976878e-06': 480,\n",
              " '1.46741423e-07': 481,\n",
              " '-8.69334016e-07': 482,\n",
              " '8.40054725e-08': 483,\n",
              " '-5.08378111e-07': 484,\n",
              " '4.73574989e-08': 485,\n",
              " '-2.92410185e-07': 486,\n",
              " '2.65615364e-08': 487,\n",
              " '-1.62655202e-07': 488,\n",
              " '1.45020894e-08': 489,\n",
              " '-8.52422549e-08': 490,\n",
              " '8.1861052e-09': 491,\n",
              " '-4.45909319e-08': 492,\n",
              " '4.75969013e-09': 493,\n",
              " '-2.58434409e-08': 494,\n",
              " '2.75781551e-09': 495,\n",
              " '-1.54917448e-08': 496,\n",
              " '1.57391509e-09': 497,\n",
              " '-8.95371398e-09': 498,\n",
              " '8.95840543e-10': 499,\n",
              " '-5.04733135e-09': 500,\n",
              " '5.14869365e-10': 501,\n",
              " '-2.88287027e-09': 502,\n",
              " '2.96438791e-10': 503,\n",
              " '-1.66392642e-09': 504,\n",
              " '-2.12589919e-10': 505,\n",
              " '1.27136716e-09': 506,\n",
              " '-1.17531388e-10': 507,\n",
              " '7.03098884e-10': 508,\n",
              " '-2.63840552e-11': 509,\n",
              " '1.57920595e-10': 510,\n",
              " '-2.03347845e-12': 511,\n",
              " '1.21752884e-11': 512,\n",
              " '-1.56683803e-13': 513,\n",
              " '9.38397325e-13': 514,\n",
              " '-1.20700924e-14': 515,\n",
              " '7.23068551e-14': 516,\n",
              " '-9.29634796e-16': 517,\n",
              " '5.57022881e-15': 518,\n",
              " '-7.15881121e-17': 519,\n",
              " '4.29023342e-16': 520,\n",
              " '-5.51196073e-18': 521,\n",
              " '3.30380757e-17': 522,\n",
              " '-6.81706004e-19': 523,\n",
              " '4.08660335e-18': 524,\n",
              " '-8.43033448e-20': 525,\n",
              " '5.05428356e-19': 526,\n",
              " '-14.0': 527,\n",
              " '0.999716142': 528,\n",
              " '-13.9991281': 529,\n",
              " '0.999432303': 530,\n",
              " '-13.9982562': 531,\n",
              " '0.981519166': 532,\n",
              " '-13.9431134': 533,\n",
              " '0.963676743': 534,\n",
              " '-13.8879555': 535,\n",
              " '0.945905053': 536,\n",
              " '-13.8327837': 537,\n",
              " '0.888468556': 538,\n",
              " '-13.6528663': 539,\n",
              " '0.841183317': 540,\n",
              " '-13.5028764': 541,\n",
              " '0.794420496': 542,\n",
              " '-13.3528474': 543,\n",
              " '0.748180188': 544,\n",
              " '-13.2028031': 545,\n",
              " '0.702462404': 546,\n",
              " '-13.0527673': 547,\n",
              " '0.612594055': 548,\n",
              " '-12.7528143': 549,\n",
              " '0.524813924': 550,\n",
              " '-12.4531717': 551,\n",
              " '0.43911921': 552,\n",
              " '-12.154017': 553,\n",
              " '0.355505919': 554,\n",
              " '-11.8555231': 555,\n",
              " '0.273968862': 556,\n",
              " '-11.5578572': 557,\n",
              " '0.0894931159': 558,\n",
              " '-10.8586351': 559,\n",
              " '-0.0835331366': 560,\n",
              " '-10.1669648': 561,\n",
              " '-0.21585776': 562,\n",
              " '-9.6115178': 563,\n",
              " '-0.340796672': 564,\n",
              " '-9.0632384': 565,\n",
              " '-0.45845166': 566,\n",
              " '-8.52299564': 567,\n",
              " '-0.568935729': 568,\n",
              " '-7.9915949': 569,\n",
              " '-0.672372239': 570,\n",
              " '-7.46977961': 571,\n",
              " '-0.768894092': 572,\n",
              " '-6.95823271': 573,\n",
              " '-0.94138825': 574,\n",
              " '-5.97068327': 575,\n",
              " '-1.08814242': 576,\n",
              " '-5.0307266': 577,\n",
              " '-1.210475': 578,\n",
              " '-4.1417971': 579,\n",
              " '-1.30978529': 580,\n",
              " '-3.30653339': 581,\n",
              " '-1.38753307': 582,\n",
              " '-2.52683518': 583,\n",
              " '-1.44521962': 584,\n",
              " '-1.80392186': 585,\n",
              " '-1.48437034': 586,\n",
              " '-1.13839061': 587,\n",
              " '-1.50644266': 588,\n",
              " '-0.533386583': 589,\n",
              " '-1.51319816': 590,\n",
              " '0.0152831388': 591,\n",
              " '-1.50612169': 592,\n",
              " '0.508527229': 593,\n",
              " '-1.48666868': 594,\n",
              " '0.947644492': 595,\n",
              " '-1.45625546': 596,\n",
              " '1.33427326': 597,\n",
              " '-1.41625088': 598,\n",
              " '1.67034294': 599,\n",
              " '-1.3679692': 600,\n",
              " '1.9580279': 601,\n",
              " '-1.29403634': 602,\n",
              " '2.26651253': 603,\n",
              " '-1.21064075': 604,\n",
              " '2.50136305': 605,\n",
              " '-1.12024581': 606,\n",
              " '2.66870709': 607,\n",
              " '-1.02509728': 608,\n",
              " '2.77487012': 609,\n",
              " '-0.927218445': 610,\n",
              " '2.82625631': 611,\n",
              " '-0.828409322': 612,\n",
              " '2.82924375': 613,\n",
              " '-0.73024913': 614,\n",
              " '2.79009377': 615,\n",
              " '-0.619260296': 616,\n",
              " '2.70016399': 617,\n",
              " '-0.512726124': 618,\n",
              " '2.57089345': 619,\n",
              " '-0.412063577': 620,\n",
              " '2.41067505': 621,\n",
              " '-0.318363587': 622,\n",
              " '2.22725472': 623,\n",
              " '-0.232418358': 624,\n",
              " '2.02767624': 625,\n",
              " '-0.154750442': 626,\n",
              " '1.81824866': 627,\n",
              " '-0.0856427385': 628,\n",
              " '1.60453247': 629,\n",
              " '-0.0251687149': 630,\n",
              " '1.39134266': 631,\n",
              " '0.03438047': 632,\n",
              " '1.14976212': 633,\n",
              " '0.0828500896': 634,\n",
              " '0.919600009': 635,\n",
              " '0.120874659': 636,\n",
              " '0.704889167': 637,\n",
              " '0.149251814': 638,\n",
              " '0.508570775': 639,\n",
              " '0.168893658': 640,\n",
              " '0.332604646': 641,\n",
              " '0.180783473': 642,\n",
              " '0.178086053': 643,\n",
              " '0.18593796': 644,\n",
              " '0.0453651584': 645,\n",
              " '0.185375093': 646,\n",
              " '-0.0658346752': 647,\n",
              " '0.17977132': 648,\n",
              " '-0.159717323': 649,\n",
              " '0.170101029': 650,\n",
              " '-0.232424475': 651,\n",
              " '0.157358931': 652,\n",
              " '-0.285670638': 653,\n",
              " '0.142449521': 654,\n",
              " '-0.321413727': 655,\n",
              " '0.126177643': 656,\n",
              " '-0.341756686': 657,\n",
              " '0.109243522': 658,\n",
              " '-0.348862513': 659,\n",
              " '0.0922416379': 660,\n",
              " '-0.344882684': 661,\n",
              " '0.0756627703': 662,\n",
              " '-0.33189862': 663,\n",
              " '0.0673521253': 664,\n",
              " '-0.322258553': 665,\n",
              " '0.0593076312': 666,\n",
              " '-0.310981991': 667,\n",
              " '0.0515676752': 668,\n",
              " '-0.298316876': 669,\n",
              " '0.0441645102': 670,\n",
              " '-0.284498257': 671,\n",
              " '0.0371245897': 672,\n",
              " '-0.269747666': 673,\n",
              " '0.0304689177': 674,\n",
              " '-0.254272661': 675,\n",
              " '0.0242134074': 676,\n",
              " '-0.238266522': 677,\n",
              " '0.0159342256': 678,\n",
              " '-0.21465436': 679,\n",
              " '0.0085241011': 680,\n",
              " '-0.190790428': 681,\n",
              " '0.00198438814': 682,\n",
              " '-0.16709403': 683,\n",
              " '-0.00369778747': 684,\n",
              " '-0.14392472': 685,\n",
              " '-0.00854738493': 686,\n",
              " '-0.121584654': 687,\n",
              " '-0.012599403': 688,\n",
              " '-0.100321513': 689,\n",
              " '-0.0158969491': 690,\n",
              " '-0.0803319065': 691,\n",
              " '-0.0184894413': 692,\n",
              " '-0.0617651493': 693,\n",
              " '-0.0204309518': 694,\n",
              " '-0.0447272885': 695,\n",
              " '-0.0221374111': 696,\n",
              " '-0.0240038715': 697,\n",
              " '-0.0228834642': 698,\n",
              " '-0.00633275599': 699,\n",
              " '-0.0228213592': 700,\n",
              " '0.00831998438': 701,\n",
              " '-0.0220994811': 702,\n",
              " '0.0200706886': 703,\n",
              " '-0.0208587093': 704,\n",
              " '0.0291002843': 705,\n",
              " '-0.0192296223': 706,\n",
              " '0.0356367134': 707,\n",
              " '-0.0173305451': 708,\n",
              " '0.0399399174': 709,\n",
              " '-0.0152662789': 710,\n",
              " '0.0422883459': 711,\n",
              " '-0.0131274788': 712,\n",
              " '0.0429674511': 713,\n",
              " '-0.0109905498': 714,\n",
              " '0.0422599913': 715,\n",
              " '-0.00832350256': 716,\n",
              " '0.0397221334': 717,\n",
              " '-0.00586682263': 718,\n",
              " '0.0358597448': 719,\n",
              " '-0.00369021815': 720,\n",
              " '0.031160625': 721,\n",
              " '-0.00183404563': 722,\n",
              " '0.0260413579': 723,\n",
              " '-0.000314073418': 724,\n",
              " '0.0208433213': 725,\n",
              " '0.000873642133': 726,\n",
              " '0.0158337329': 727,\n",
              " '0.00174808148': 728,\n",
              " '0.0112096966': 729,\n",
              " '0.00233890189': 730,\n",
              " '0.00710482292': 731,\n",
              " '0.00249922691': 732,\n",
              " '0.00567240657': 733,\n",
              " '0.00262472172': 734,\n",
              " '0.00433269243': 735,\n",
              " '0.00271773811': 736,\n",
              " '0.00308727041': 737,\n",
              " '0.00278065547': 738,\n",
              " '0.00193684308': 739,\n",
              " '0.00281586096': 740,\n",
              " '0.000881303226': 741,\n",
              " '0.00282572956': 742,\n",
              " '-8.01860403e-05': 743,\n",
              " '0.0028126069': 744,\n",
              " '-0.000949115545': 745,\n",
              " '0.0027633262': 746,\n",
              " '-0.00196718893': 747,\n",
              " '0.00268255233': 748,\n",
              " '-0.0028307868': 749,\n",
              " '0.00257533537': 750,\n",
              " '-0.00354750533': 751,\n",
              " '0.0024464527': 752,\n",
              " '-0.00412599098': 753,\n",
              " '0.00230037804': 754,\n",
              " '-0.00457569021': 755,\n",
              " '0.00214125854': 756,\n",
              " '-0.00490662078': 757,\n",
              " '0.00189452037': 758,\n",
              " '-0.00519714803': 759,\n",
              " '0.00163855412': 760,\n",
              " '-0.00529119625': 761,\n",
              " '0.00138213902': 762,\n",
              " '-0.00522092938': 763,\n",
              " '0.00113251066': 764,\n",
              " '-0.00501738917': 765,\n",
              " '0.000895435088': 766,\n",
              " '-0.00470982559': 767,\n",
              " '0.000674607943': 768,\n",
              " '-0.00432384282': 769,\n",
              " '0.000474016767': 770,\n",
              " '-0.00388490827': 771,\n",
              " '0.000295704749': 772,\n",
              " '-0.00341474581': 773,\n",
              " '0.000140726796': 774,\n",
              " '-0.00293210113': 775,\n",
              " '9.29528825e-06': 776,\n",
              " '-0.00245274467': 777,\n",
              " '-0.000142635425': 778,\n",
              " '-0.00177883091': 779,\n",
              " '-0.000248289542': 780,\n",
              " '-0.0011700801': 781,\n",
              " '-0.00031307477': 782,\n",
              " '-0.000645511332': 783,\n",
              " '-0.000343446322': 784,\n",
              " '-0.000214485615': 785,\n",
              " '-0.000346240931': 786,\n",
              " '0.000121160817': 787,\n",
              " '-0.000328199011': 788,\n",
              " '0.000365820099': 789,\n",
              " '-0.000307571438': 790,\n",
              " '0.000483059344': 791,\n",
              " '-0.000282116629': 792,\n",
              " '0.000566435565': 793,\n",
              " '-0.000253381922': 794,\n",
              " '0.000619613635': 795,\n",
              " '-0.000222732487': 796,\n",
              " '0.00064643834': 797,\n",
              " '-0.000191346256': 798,\n",
              " '0.000650798626': 799,\n",
              " '-0.000163127766': 800,\n",
              " '0.000638570082': 801,\n",
              " '-0.000135727456': 802,\n",
              " '0.000613791833': 803,\n",
              " '-0.000109636781': 804,\n",
              " '0.0005790395': 805,\n",
              " '-8.52385864e-05': 806,\n",
              " '0.000536702729': 807,\n",
              " '-6.28158579e-05': 808,\n",
              " '0.000488959547': 809,\n",
              " '-3.88174755e-05': 810,\n",
              " '0.000427360866': 811,\n",
              " '-1.81091675e-05': 812,\n",
              " '0.000363705362': 813,\n",
              " '-7.32943621e-07': 814,\n",
              " '0.000300422317': 815,\n",
              " '1.33839352e-05': 816,\n",
              " '0.000239467816': 817,\n",
              " '2.44048475e-05': 818,\n",
              " '0.000182355518': 819,\n",
              " '3.74655855e-05': 820,\n",
              " '9.01340309e-05': 821,\n",
              " '4.25723831e-05': 822,\n",
              " '1.85451434e-05': 823,\n",
              " '4.17614477e-05': 824,\n",
              " '-3.18739273e-05': 825,\n",
              " '3.7018134e-05': 826,\n",
              " '-6.29360027e-05': 827,\n",
              " '3.00796875e-05': 828,\n",
              " '-7.76592844e-05': 829,\n",
              " '2.23533898e-05': 830,\n",
              " '-7.98526503e-05': 831,\n",
              " '1.89567503e-05': 832,\n",
              " '-7.78168142e-05': 833,\n",
              " '1.56791069e-05': 834,\n",
              " '-7.4382642e-05': 835,\n",
              " '1.25737861e-05': 836,\n",
              " '-6.98509065e-05': 837,\n",
              " '9.68203714e-06': 838,\n",
              " '-6.44953358e-05': 839,\n",
              " '7.03386918e-06': 840,\n",
              " '-5.85640888e-05': 841,\n",
              " '1.7313971e-06': 842,\n",
              " '-4.3094523e-05': 843,\n",
              " '-1.95877371e-06': 844,\n",
              " '-2.79781595e-05': 845,\n",
              " '-4.16617774e-06': 846,\n",
              " '-1.48421257e-05': 847,\n",
              " '-5.14629166e-06': 848,\n",
              " '-4.49638883e-06': 849,\n",
              " '-5.20580953e-06': 850,\n",
              " '2.83276742e-06': 851,\n",
              " '-4.4112432e-06': 852,\n",
              " '8.17699181e-06': 853,\n",
              " '-3.16154411e-06': 854,\n",
              " '9.81792389e-06': 855,\n",
              " '-1.87091204e-06': 856,\n",
              " '9.01292519e-06': 857,\n",
              " '-7.86927044e-07': 858,\n",
              " '6.94544729e-06': 859,\n",
              " '-1.52919025e-08': 860,\n",
              " '4.50642656e-06': 861,\n",
              " '2.08241622e-07': 862,\n",
              " '3.55621058e-06': 863,\n",
              " '3.80352827e-07': 864,\n",
              " '2.65287128e-06': 865,\n",
              " '5.04494415e-07': 866,\n",
              " '1.83412236e-06': 867,\n",
              " '5.85830218e-07': 868,\n",
              " '1.11267129e-06': 869,\n",
              " '6.38608349e-07': 870,\n",
              " '3.15020497e-07': 871,\n",
              " '6.39177166e-07': 872,\n",
              " '-2.99633542e-07': 873,\n",
              " '6.0065666e-07': 874,\n",
              " '-7.4237776e-07': 875,\n",
              " '3.8134677e-07': 876,\n",
              " '-1.22068507e-06': 877,\n",
              " '1.42829514e-07': 878,\n",
              " '-1.05174284e-06': 879,\n",
              " '-1.98401369e-08': 880,\n",
              " '-5.09398878e-07': 881,\n",
              " '-7.84311034e-08': 882,\n",
              " '-9.69664826e-08': 883,\n",
              " '-8.14314514e-08': 884,\n",
              " '3.56414038e-08': 885,\n",
              " '-7.4339368e-08': 886,\n",
              " '1.09315972e-07': 887,\n",
              " '-6.18754867e-08': 888,\n",
              " '1.45437313e-07': 889,\n",
              " '-4.07530909e-08': 890,\n",
              " '1.5546058e-07': 891,\n",
              " '-2.05458042e-08': 892,\n",
              " '1.32401179e-07': 893,\n",
              " '-4.64864775e-09': 894,\n",
              " '9.40608642e-08': 895,\n",
              " '3.25539037e-09': 896,\n",
              " '6.32621566e-08': 897,\n",
              " '1.49816415e-08': 898,\n",
              " '-1.22632967e-08': 899,\n",
              " '1.02126052e-08': 900,\n",
              " '-3.52642756e-08': 901,\n",
              " '2.36192253e-09': 902,\n",
              " '-2.54430972e-08': 903,\n",
              " '-8.28700738e-10': 904,\n",
              " '-3.92648847e-09': 905,\n",
              " '-5.23044389e-10': 906,\n",
              " '3.76150999e-10': 907,\n",
              " '-6.55682536e-11': 908,\n",
              " '2.31401209e-10': 909,\n",
              " '-2.11998846e-12': 910,\n",
              " '3.20934888e-11': 911,\n",
              " '2.3610081e-12': 912,\n",
              " '2.33392163e-12': 913,\n",
              " '4.76179405e-13': 914,\n",
              " '-9.81710743e-13': 915,\n",
              " '3.0': 916,\n",
              " '1.00006082': 917,\n",
              " '2.99949311': 918,\n",
              " '1.00012163': 919,\n",
              " '2.99898624': 920,\n",
              " '1.0039464': 921,\n",
              " '2.96698473': 922,\n",
              " '1.0077302': 923,\n",
              " '2.93508369': 924,\n",
              " '1.01147316': 925,\n",
              " '2.90328326': 926,\n",
              " '1.0233915': 927,\n",
              " '2.80032862': 928,\n",
              " '1.03299554': 929,\n",
              " '2.71536217': 930,\n",
              " '1.04230499': 931,\n",
              " '2.63114598': 932,\n",
              " '1.05132246': 933,\n",
              " '2.54768283': 934,\n",
              " '1.06005058': 935,\n",
              " '2.46497539': 936,\n",
              " '1.07664934': 937,\n",
              " '2.30183745': 938,\n",
              " '1.09212245': 939,\n",
              " '2.1417503': 940,\n",
              " '1.10649122': 941,\n",
              " '1.98472964': 942,\n",
              " '1.11977705': 943,\n",
              " '1.83078889': 944,\n",
              " '1.13200144': 945,\n",
              " '1.67993916': 946,\n",
              " '1.15679662': 947,\n",
              " '1.33578285': 948,\n",
              " '1.17607006': 949,\n",
              " '1.00900379': 950,\n",
              " '1.1878674': 951,\n",
              " '0.756151879': 952,\n",
              " '1.19635797': 953,\n",
              " '0.514764631': 954,\n",
              " '1.20169488': 955,\n",
              " '0.284791562': 956,\n",
              " '1.20403039': 957,\n",
              " '0.0661589357': 958,\n",
              " '1.20351564': 959,\n",
              " '-0.141228833': 960,\n",
              " '1.20030035': 961,\n",
              " '-0.337487832': 962,\n",
              " '1.18640272': 963,\n",
              " '-0.696373773': 964,\n",
              " '1.16349319': 965,\n",
              " '-1.01279165': 966,\n",
              " '1.13268579': 967,\n",
              " '-1.28826422': 968,\n",
              " '1.0950511': 969,\n",
              " '-1.52451949': 970,\n",
              " '1.05161117': 971,\n",
              " '-1.72345498': 972,\n",
              " '1.00333555': 973,\n",
              " '-1.88710369': 974,\n",
              " '0.951138033': 975,\n",
              " '-2.01760264': 976,\n",
              " '0.896177767': 977,\n",
              " '-2.11670756': 978,\n",
              " '0.838967519': 979,\n",
              " '-2.1874288': 980,\n",
              " '0.780231247': 981,\n",
              " '-2.23201213': 982,\n",
              " '0.720633213': 983,\n",
              " '-2.25270415': 984,\n",
              " '0.660778229': 985,\n",
              " '-2.2517327': 986,\n",
              " '0.601212386': 987,\n",
              " '-2.23128938': 988,\n",
              " '0.542424232': 989,\n",
              " '-2.19351416': 990,\n",
              " '0.467074356': 991,\n",
              " '-2.12099833': 992,\n",
              " '0.394648971': 993,\n",
              " '-2.02663254': 994,\n",
              " '0.325835779': 995,\n",
              " '-1.91465586': 996,\n",
              " '0.261179587': 997,\n",
              " '-1.78901497': 998,\n",
              " '0.201092883': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Sf-VRQHVUz",
        "outputId": "f40fe1bf-8f1c-43bf-f908-f9399dd1f626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o6WNS4AIHVUz"
      },
      "outputs": [],
      "source": [
        "# text = (\n",
        "#         'Hello, how are you? I am Romeo.\\n'\n",
        "#         'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
        "#         'Nice meet you too. How are you today?\\n'\n",
        "#         'Great. My baseball team won the competition.\\n'\n",
        "#         'Oh Congratulations, Juliet\\n'\n",
        "#         'Thank you Romeo'\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jtlZlMqhHVUz"
      },
      "outputs": [],
      "source": [
        "# sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')\n",
        "# word_list = list(set(\" \".join(sentences).split()))\n",
        "# word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "\n",
        "\n",
        "# for i, w in enumerate(word_list):\n",
        "#     word_dict[w] = i + 4\n",
        "# number_dict = {i: w for i, w in enumerate(word_dict)}\n",
        "# vocab_size = len(word_dict)\n",
        "\n",
        "# token_list = list()\n",
        "# for sentence in sentences:\n",
        "#     arr = [word_dict[s] for s in sentence.split()]\n",
        "#     token_list.append(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1Rn7U3V3HVU0"
      },
      "outputs": [],
      "source": [
        "# token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Sf5LkQy7HVU0"
      },
      "outputs": [],
      "source": [
        "# maxlen = 30\n",
        "# batch_size = 6\n",
        "# max_pred = 5\n",
        "# n_layers = 6\n",
        "# n_heads = 12\n",
        "# d_model = 768\n",
        "# d_ff = 768 * 4\n",
        "# d_k = d_v = 64\n",
        "# n_segments = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bm_3iEF8HVU0"
      },
      "outputs": [],
      "source": [
        "batch_size = 6\n",
        "max_pred = 5\n",
        "n_layers = 6\n",
        "n_heads = 12\n",
        "d_model = 32\n",
        "d_ff = 32 * 4\n",
        "d_k = d_v = 2\n",
        "n_segments = 6\n",
        "vocab_size = 50000\n",
        "maxlen = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "x1wdn_rxHVU0"
      },
      "outputs": [],
      "source": [
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index= randrange(len(tokenized_series)), randrange(len(tokenized_series))\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "\n",
        "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
        "\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15))))\n",
        "\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.8:\n",
        "                input_ids[pos] = word_dict['[MASK]']\n",
        "            elif random() < 0.5:\n",
        "                index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = index\n",
        "\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WKpbYzIzHVU0"
      },
      "outputs": [],
      "source": [
        "# def make_batch():\n",
        "#     batch = []\n",
        "#     positive = negative = 0\n",
        "#     while positive != batch_size/2 or negative != batch_size/2:\n",
        "#         tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
        "#         tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "\n",
        "#         input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
        "\n",
        "#         segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "#         n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15))))\n",
        "\n",
        "#         cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "#                           if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
        "#         shuffle(cand_maked_pos)\n",
        "#         masked_tokens, masked_pos = [], []\n",
        "#         for pos in cand_maked_pos[:n_pred]:\n",
        "#             masked_pos.append(pos)\n",
        "#             masked_tokens.append(input_ids[pos])\n",
        "#             if random() < 0.8:\n",
        "#                 input_ids[pos] = word_dict['[MASK]']\n",
        "#             elif random() < 0.5:\n",
        "#                 index = randint(0, vocab_size - 1)\n",
        "#                 input_ids[pos] = word_dict[number_dict[index]]\n",
        "\n",
        "#         n_pad = maxlen - len(input_ids)\n",
        "#         input_ids.extend([0] * n_pad)\n",
        "#         segment_ids.extend([0] * n_pad)\n",
        "\n",
        "#         if max_pred > n_pred:\n",
        "#             n_pad = max_pred - n_pred\n",
        "#             masked_tokens.extend([0] * n_pad)\n",
        "#             masked_pos.extend([0] * n_pad)\n",
        "\n",
        "#         if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "#             batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "#             positive += 1\n",
        "#         elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "#             batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "#             negative += 1\n",
        "#     return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PqyuLen8HVU1"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
        "    pad_attn_mask = pad_attn_mask.to(device)\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "E5FM5adKHVU1"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TujIQEWHVU1",
        "outputId": "3be3747e-db33-42f4-9989-8920e728f708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "batch = make_batch()\n",
        "print(\"done\", type(batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MdBoj3MhHVU1"
      },
      "outputs": [],
      "source": [
        "def flatten_tokens(tokens):\n",
        "    flattened_tokens = []\n",
        "    for nums in tokens:\n",
        "        if type(nums) is list:\n",
        "            flattened_tokens.extend(nums)\n",
        "        else:\n",
        "            flattened_tokens.append(nums)\n",
        "\n",
        "    return flattened_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PUZW6HchHVU1"
      },
      "outputs": [],
      "source": [
        "def arrange_tokens(tokens):\n",
        "    arranged = []\n",
        "    for nums in tokens:\n",
        "        flattened = flatten_tokens(nums)\n",
        "        length = len(flattened)\n",
        "        if length < max_length:\n",
        "            missing = max_length - length\n",
        "            flattened.extend([0] * missing)\n",
        "        elif length > max_length:\n",
        "            flattened = flattened[:max_length]\n",
        "        arranged.append(flattened)\n",
        "\n",
        "    return arranged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eCjT60C4HVU1"
      },
      "outputs": [],
      "source": [
        "# def flatten_tokens(nestedList):\n",
        "# \tif not(bool(nestedList)):\n",
        "# \t\treturn nestedList\n",
        "\n",
        "# \tif isinstance(nestedList[0], list):\n",
        "# \t\treturn flatten_tokens(*nestedList[:1]) + flatten_tokens(nestedList[1:])\n",
        "\n",
        "# \treturn nestedList[:1] + flatten_tokens(nestedList[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xwt9Is2nHVU2"
      },
      "outputs": [],
      "source": [
        "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "# input_ids = input_ids.to(device)\n",
        "# segment_ids = segment_ids.to(device)\n",
        "# masked_tokens = masked_tokens.to(device)\n",
        "# masked_pos = masked_pos.to(device)\n",
        "# isNext = isNext.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CWeln5pEHVU2"
      },
      "outputs": [],
      "source": [
        "# # input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "# input_ids = torch.LongTensor(arrange_tokens([nums[0] for nums in batch]))\n",
        "# segment_ids = torch.LongTensor([nums[1] for nums in batch])\n",
        "# masked_tokens = torch.LongTensor(arrange_tokens([nums[2] for nums in batch]))\n",
        "# masked_pos = torch.LongTensor([nums[3] for nums in batch])\n",
        "# isNext = torch.LongTensor([nums[4] for nums in batch])\n",
        "\n",
        "# # input_ids = input_ids.to(device)\n",
        "# # segment_ids = segment_ids.to(device)\n",
        "# # masked_tokens = masked_tokens.to(device)\n",
        "# # masked_pos = masked_pos.to(device)\n",
        "# # isNext = isNext.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KCY_CnANXb6m"
      },
      "outputs": [],
      "source": [
        "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "input_ids = torch.LongTensor(arrange_tokens([nums[0] for nums in batch]))\n",
        "segment_ids = torch.LongTensor(arrange_tokens([nums[1] for nums in batch]))\n",
        "masked_tokens = torch.LongTensor(arrange_tokens([nums[2] for nums in batch]))\n",
        "masked_pos = torch.LongTensor(arrange_tokens([nums[3] for nums in batch]))\n",
        "isNext = torch.LongTensor([nums[4] for nums in batch])\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "segment_ids = segment_ids.to(device)\n",
        "masked_tokens = masked_tokens.to(device)\n",
        "masked_pos = masked_pos.to(device)\n",
        "isNext = isNext.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VM3cV8cHVU2",
        "outputId": "e3dfbfc7-bdda-4910-d532-bef9c6663a00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        device='cuda:0'),\n",
              " tensor([    1,     3,     4, 10870, 14353, 14354, 14355, 14356, 14357, 14358,\n",
              "         14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368,\n",
              "         14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378,\n",
              "         14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388,\n",
              "         14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398,\n",
              "         14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408,\n",
              "         14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418,\n",
              "         14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428,\n",
              "         14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438,\n",
              "         14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448,\n",
              "         14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458,\n",
              "         14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468,\n",
              "         14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478,\n",
              "         14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488,\n",
              "         14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498,\n",
              "         14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508,\n",
              "         14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518,\n",
              "         14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528,\n",
              "         14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538,\n",
              "         14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548,\n",
              "         14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558,\n",
              "         14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568,\n",
              "         14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578,\n",
              "         14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588,\n",
              "         14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598,\n",
              "         14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608,\n",
              "         14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618,\n",
              "         14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628,\n",
              "         14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638,\n",
              "         14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648,\n",
              "         14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658,\n",
              "         14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668,\n",
              "         14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678,\n",
              "         14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688,\n",
              "         14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698,\n",
              "         14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708,\n",
              "         14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718,\n",
              "         14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728,\n",
              "         14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738,\n",
              "         14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748,\n",
              "         14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758,\n",
              "         14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768,\n",
              "         14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778,\n",
              "         14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788,\n",
              "         14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798,\n",
              "         14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808,\n",
              "         14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818,\n",
              "         14819, 14820, 14821, 14822,     2,  1902,  1902,     2,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "        device='cuda:0'),\n",
              " tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
              " tensor([    4,  3510, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890,\n",
              "         13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900,\n",
              "         13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910,\n",
              "         13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920,\n",
              "         13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930,\n",
              "         13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940,\n",
              "         13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950,\n",
              "         13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960,\n",
              "         13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970,\n",
              "         13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980,\n",
              "         13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990,\n",
              "         13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000,\n",
              "         14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010,\n",
              "         14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020,\n",
              "         14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030,\n",
              "         14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040,\n",
              "         14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050,\n",
              "         14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060,\n",
              "         14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070,\n",
              "         14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080,\n",
              "         14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090,\n",
              "         14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100,\n",
              "         14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110,\n",
              "         14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120,\n",
              "         14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130,\n",
              "         14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140,\n",
              "         14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150,\n",
              "         14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160,\n",
              "         14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170,\n",
              "         14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180,\n",
              "         14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190,\n",
              "         14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200,\n",
              "         14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210,\n",
              "         14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220,\n",
              "         14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230,\n",
              "         14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240,\n",
              "         14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250,\n",
              "         14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260,\n",
              "         14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270,\n",
              "         14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280,\n",
              "         14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290,\n",
              "         14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300,\n",
              "         14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310,\n",
              "         14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320,\n",
              "         14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330,\n",
              "         14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340,\n",
              "         14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350,\n",
              "         14351, 14352,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "        device='cuda:0'),\n",
              " tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
              " tensor(0, device='cuda:0'))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_attn_pad_mask(input_ids, input_ids)[0][0], input_ids[0], segment_ids[0], masked_tokens[0], masked_pos[0], isNext[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9AE037G_HVU2"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        # print(\"tok_embed:\", self.tok_embed.embedding_dim, self.tok_embed.num_embeddings)\n",
        "        # print(\"pos_embed:\",  self.pos_embed.embedding_dim, self.pos_embed.num_embeddings)\n",
        "        # print(\"seg_embed:\", self.seg_embed.embedding_dim, self.seg_embed.num_embeddings)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)\n",
        "        # print(\"x:\", x.size())\n",
        "        # print(\"pos:\", pos.size())\n",
        "        # print(\"seg:\", seg.size())\n",
        "        x = x.to(device)\n",
        "        pos = pos.to(device)\n",
        "        seg = seg.to(device)\n",
        "        embedding = self.tok_embed(x) \n",
        "        embedding += self.pos_embed(pos)\n",
        "        embedding += self.seg_embed(seg)\n",
        "        embedding = embedding.to(device)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jDv4HiKMeAHu"
      },
      "outputs": [],
      "source": [
        "# embedding = Embedding()\n",
        "# x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Token indices\n",
        "# seg = torch.tensor([[0, 0, 1], [1, 1, 0]])  # Segment indices\n",
        "\n",
        "# output = embedding(x, seg)\n",
        "# print(output.shape)  # Output shape: (2, 3, 512)\n",
        "# output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_pVJhuvyeOpv"
      },
      "outputs": [],
      "source": [
        "# output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jRcq_G0UHVU2"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        scores = scores.to(device)\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        attn = attn.to(device)\n",
        "        context = torch.matmul(attn, V)\n",
        "        context = context.to(device)\n",
        "        return scores, context, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGPbvtKNHVU2",
        "outputId": "e5be8db0-9b3f-47e6-8b54-39a6e3c95080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6, 1000, 32])\n",
            "Masks: tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "       device='cuda:0')\n",
            "\n",
            "Scores: tensor([ 2.2627e+01,  1.2051e+01,  7.3366e+00,  1.0571e+01,  3.6721e+00,\n",
            "         9.5964e-01, -2.2476e+00,  8.4074e+00,  5.9064e+00,  4.9088e+00,\n",
            "         9.3431e+00,  6.0240e+00,  7.3285e+00,  8.7398e+00,  5.8452e+00,\n",
            "         9.2424e+00,  2.6154e+00,  9.6182e+00,  1.0485e+01,  7.1492e+00,\n",
            "         8.6818e+00,  8.8851e+00,  6.4812e+00,  1.0249e+01,  4.6464e+00,\n",
            "         7.5520e+00,  7.7036e+00,  7.6340e+00,  5.7878e+00,  1.0338e+01,\n",
            "         1.0075e+01,  7.0397e+00,  3.4361e+00,  7.0907e+00,  1.0236e+01,\n",
            "         1.0811e+01,  6.8132e+00,  1.0523e+00,  3.6675e+00,  1.2380e+01,\n",
            "         1.3353e+01,  7.9997e+00,  2.5806e+00,  1.1518e+01,  6.2214e+00,\n",
            "         1.1070e+01,  1.1457e+01,  5.9358e+00,  1.9305e+00,  2.4292e+00,\n",
            "         6.4598e+00,  5.6178e+00,  7.5225e+00,  1.0078e+01,  3.8473e+00,\n",
            "         8.7182e+00,  4.1518e+00,  3.4440e+00,  7.2998e+00,  7.5835e+00,\n",
            "         7.5702e+00,  9.0051e+00,  1.5989e+00,  8.4972e+00,  8.5453e+00,\n",
            "         3.6765e+00,  1.0648e+01,  8.1387e+00,  1.0292e+01,  8.2097e+00,\n",
            "         1.3139e+00,  6.3797e+00,  5.5164e+00,  4.6333e+00,  2.0026e+00,\n",
            "         5.6841e+00,  6.8964e+00,  2.8727e+00,  3.8549e+00,  7.2067e+00,\n",
            "         3.9812e+00,  6.1908e+00,  1.1580e+01,  6.9080e+00,  1.3628e+01,\n",
            "         8.9370e+00,  9.4791e+00,  9.9959e+00,  9.6298e+00,  6.2040e+00,\n",
            "         3.8248e+00,  1.1127e+01,  9.9924e+00,  2.5594e+00,  4.0007e+00,\n",
            "         1.0812e+01,  3.9303e+00,  7.0873e+00,  9.5891e+00,  8.7946e+00,\n",
            "         9.7389e+00,  1.0631e+01,  3.1978e+00,  6.4993e+00,  8.5266e+00,\n",
            "         9.1166e+00,  8.1291e+00,  9.0866e+00,  4.6985e+00,  6.4647e+00,\n",
            "         3.2569e-01,  6.7309e+00,  1.1579e+01,  6.2948e+00,  1.0985e+01,\n",
            "         1.2448e+01,  4.3821e+00,  8.0954e+00,  8.7180e+00,  5.4910e+00,\n",
            "         1.1335e+01, -5.7036e-01,  8.6325e+00,  8.3372e+00,  4.9338e+00,\n",
            "         7.9313e+00,  6.4982e+00,  4.9229e+00,  8.1184e+00, -7.6635e-01,\n",
            "         6.1523e+00,  3.7910e+00,  2.6398e+00,  3.6941e+00,  9.7131e+00,\n",
            "         7.1989e+00,  9.5657e+00,  1.0185e+01,  1.2123e+01,  5.1553e+00,\n",
            "         1.2456e+01,  8.2413e+00,  1.3879e+01,  6.6156e+00,  7.1795e+00,\n",
            "         1.4304e+01,  1.7036e+00,  1.1290e+01,  8.2033e+00,  9.3266e+00,\n",
            "         4.8434e+00,  1.1066e+01,  7.8509e+00,  1.1574e+00,  3.7269e+00,\n",
            "         1.2719e+01,  9.8618e+00,  6.6772e+00,  3.9338e+00,  9.9238e+00,\n",
            "        -6.9740e-01,  1.1742e+01,  7.7525e+00,  6.1476e+00,  1.0241e+01,\n",
            "         9.0055e+00,  7.6592e+00,  5.9587e+00,  8.3164e+00,  1.1773e+01,\n",
            "        -3.2653e-01,  8.2319e+00,  7.4113e+00,  7.1213e+00,  3.6843e+00,\n",
            "         1.2645e+01,  1.7481e+00,  4.7547e+00,  1.0223e+01,  7.5875e+00,\n",
            "         4.6169e+00,  7.0978e+00,  1.2200e+01,  2.4673e+00,  4.8182e+00,\n",
            "         4.7301e+00,  9.9341e+00,  6.1587e+00,  9.2808e+00,  6.8424e+00,\n",
            "         2.9279e+00,  5.8245e+00,  1.0810e+01,  1.1100e+01,  9.0822e+00,\n",
            "         7.6780e+00,  8.3100e+00,  9.5511e+00,  7.8967e+00,  6.4815e+00,\n",
            "         7.4664e+00,  5.0797e+00,  9.5145e+00,  3.4474e+00,  4.9618e+00,\n",
            "         5.9434e+00,  1.3025e+01,  5.1646e+00,  2.3438e+00,  6.5778e+00,\n",
            "         9.8621e+00,  7.6950e+00,  7.4281e+00,  9.5989e+00,  5.5148e+00,\n",
            "         7.7557e+00,  4.8989e+00,  8.1111e+00,  3.7766e+00,  8.7872e+00,\n",
            "         7.9157e+00,  9.0440e+00,  3.3483e+00,  8.9715e+00,  1.2907e+00,\n",
            "         3.4926e+00,  7.1731e+00,  5.7943e+00,  4.9910e+00,  7.2787e+00,\n",
            "         6.5475e+00,  5.7149e+00,  4.6459e+00,  1.0965e+01,  9.6006e+00,\n",
            "         3.0440e+00,  6.0712e+00,  7.2718e+00,  1.1032e+01,  9.3462e+00,\n",
            "         5.2565e+00,  6.3470e+00,  6.5809e+00,  8.6598e+00,  1.0983e+01,\n",
            "         4.3093e+00,  1.0328e+01,  4.6019e+00,  9.0192e+00,  4.2674e+00,\n",
            "         7.5848e+00,  6.1217e+00,  4.8585e+00,  4.6319e+00,  1.1008e+01,\n",
            "         1.6494e+00,  5.2379e+00,  4.8457e+00,  6.9863e+00,  1.1047e+01,\n",
            "         1.0484e+01,  8.4173e+00,  3.1857e+00,  9.1689e+00,  8.0471e+00,\n",
            "         1.2877e+01,  1.1175e+01,  1.1087e+01,  4.0126e+00,  6.0076e+00,\n",
            "         1.1447e+01,  6.5156e+00,  8.0101e+00,  6.2843e+00,  8.0877e+00,\n",
            "         6.1020e+00,  2.1112e+00,  7.6690e+00,  1.3718e+01,  8.5405e+00,\n",
            "         9.3062e+00,  1.0925e+01,  1.3864e+01,  9.7444e+00,  1.0086e+01,\n",
            "         6.3507e+00,  1.6269e+01,  7.3368e+00,  2.6992e+00,  6.1307e+00,\n",
            "         1.2750e+01,  6.6227e+00,  1.0814e+01,  3.6531e+00,  1.1193e+01,\n",
            "         7.9129e+00,  1.1374e+01,  5.8302e+00,  7.5754e+00,  1.0260e+01,\n",
            "         3.6356e+00,  6.8289e+00,  5.1383e+00,  9.7272e+00,  4.9472e+00,\n",
            "         1.2577e+01,  5.4489e+00,  8.5386e+00,  5.8204e+00,  7.2596e+00,\n",
            "         1.4318e+00,  2.8641e+00,  6.8912e+00,  7.2187e+00,  9.1355e+00,\n",
            "         9.6687e+00,  6.9787e+00,  9.5420e+00,  1.0266e+01,  5.9073e+00,\n",
            "         9.3204e+00,  5.4376e+00,  9.8428e+00,  4.6904e+00,  5.9727e+00,\n",
            "         3.2059e+00,  2.2409e+00,  4.8961e+00,  1.3346e+01,  5.2313e+00,\n",
            "         1.0216e+01,  9.9853e+00,  7.1010e+00,  1.2775e+01,  7.1443e+00,\n",
            "         5.7283e+00,  1.1135e+01,  9.8604e+00,  8.7902e+00,  6.5847e+00,\n",
            "         1.4201e+01,  7.4992e+00,  3.3253e+00,  1.0895e+01,  7.0579e+00,\n",
            "         4.7249e+00,  9.8542e+00,  3.7899e+00,  1.0266e+01,  6.9311e+00,\n",
            "         7.5345e+00,  5.7928e+00,  3.5608e+00,  1.1039e+01,  7.6050e+00,\n",
            "         4.8464e+00,  8.6088e+00,  6.0204e+00,  8.3618e+00,  2.3871e+00,\n",
            "         6.3093e+00,  8.2094e+00,  4.1520e+00,  6.3749e+00, -8.8091e-01,\n",
            "         5.6663e+00,  9.1908e+00,  2.5778e+00,  7.7085e+00,  1.3043e+01,\n",
            "         3.0372e+00,  7.8408e+00,  1.2760e+01,  6.2603e+00,  2.7514e+00,\n",
            "         8.4816e+00,  1.0465e+01,  1.1559e+01,  8.6605e+00,  1.2108e+01,\n",
            "         4.7180e+00,  2.6988e+00,  4.7053e+00,  6.1565e+00,  8.9027e-01,\n",
            "         8.7903e+00,  1.0313e+01,  8.3146e+00,  1.2646e+01,  6.9755e+00,\n",
            "         9.0763e+00,  6.8626e+00,  4.9102e+00,  4.7487e+00,  9.3250e+00,\n",
            "         5.4419e+00,  7.0863e+00,  1.0701e+01,  6.7980e+00,  8.4580e+00,\n",
            "         7.1192e+00,  3.9058e+00,  1.0288e+01,  7.7926e+00,  5.5078e+00,\n",
            "         3.5643e+00,  1.1309e+01,  9.3416e+00,  8.7431e+00,  6.0028e+00,\n",
            "         6.3487e+00,  4.9491e+00,  8.7260e-01,  5.9011e+00,  6.2033e+00,\n",
            "         8.8904e+00,  1.0388e+01,  1.0016e+01,  7.5262e+00,  7.9745e+00,\n",
            "         7.1178e+00,  5.7159e+00,  9.8429e+00,  1.0115e+01,  7.0349e+00,\n",
            "         7.0250e+00, -1.7365e+00, -2.4273e+00,  1.2718e+01,  6.0397e+00,\n",
            "         6.6003e+00,  1.0338e+01,  8.8991e+00,  9.9868e+00,  9.4619e+00,\n",
            "         2.8499e+00,  6.7613e+00,  5.4277e+00,  1.2085e+01,  4.6580e+00,\n",
            "         9.4628e+00,  8.9139e+00,  9.5942e+00,  1.2580e+01,  7.1455e+00,\n",
            "         1.2732e+01,  5.3453e+00,  4.9212e+00,  8.2854e+00,  1.0475e+01,\n",
            "         7.3239e+00,  4.4161e+00,  9.8696e+00,  8.9669e+00,  4.6581e+00,\n",
            "         7.4583e+00,  7.0530e+00,  6.4823e-01,  7.0101e+00,  8.6136e+00,\n",
            "         6.3567e+00,  7.8933e+00,  3.5064e+00,  1.1622e+01,  5.5662e+00,\n",
            "         8.2097e+00,  1.0750e+01,  1.0499e+01,  3.0940e-01,  1.0097e+01,\n",
            "         1.0427e+01,  3.9461e+00,  5.6836e+00,  6.2945e+00,  6.5790e+00,\n",
            "         9.2916e+00,  9.3775e+00,  6.3712e+00, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
            "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>) \n",
            "\n",
            "Attention M: tensor([9.9543e-01, 2.5393e-05, 2.2768e-07, 5.7834e-06, 5.8328e-09, 3.8712e-10,\n",
            "        1.5665e-11, 6.6433e-07, 5.4476e-08, 2.0088e-08, 1.6932e-06, 6.1276e-08,\n",
            "        2.2584e-07, 9.2627e-07, 5.1241e-08, 1.5311e-06, 2.0275e-09, 2.2296e-06,\n",
            "        5.3037e-06, 1.8877e-07, 8.7408e-07, 1.0711e-06, 9.6788e-08, 4.1876e-06,\n",
            "        1.5452e-08, 2.8240e-07, 3.2863e-07, 3.0654e-07, 4.8382e-08, 4.5794e-06,\n",
            "        3.5208e-06, 1.6919e-07, 4.6065e-09, 1.7805e-07, 4.1340e-06, 7.3478e-06,\n",
            "        1.3491e-07, 4.2470e-10, 5.8060e-09, 3.5291e-05, 9.3408e-05, 4.4189e-07,\n",
            "        1.9581e-09, 1.4910e-05, 7.4648e-08, 9.5173e-06, 1.4026e-05, 5.6099e-08,\n",
            "        1.0221e-09, 1.6829e-09, 9.4737e-08, 4.0820e-08, 2.7420e-07, 3.5302e-06,\n",
            "        6.9493e-09, 9.0643e-07, 9.4231e-09, 4.6428e-09, 2.1946e-07, 2.9143e-07,\n",
            "        2.8760e-07, 1.2077e-06, 7.3362e-10, 7.2674e-07, 7.6255e-07, 5.8584e-09,\n",
            "        6.2410e-06, 5.0775e-07, 4.3721e-06, 5.4514e-07, 5.5171e-10, 8.7453e-08,\n",
            "        3.6884e-08, 1.5251e-08, 1.0985e-09, 4.3616e-08, 1.4661e-07, 2.6222e-09,\n",
            "        7.0020e-09, 1.9994e-07, 7.9451e-09, 7.2399e-08, 1.5850e-05, 1.4832e-07,\n",
            "        1.2291e-04, 1.1282e-06, 1.9399e-06, 3.2529e-06, 2.2555e-06, 7.3359e-08,\n",
            "        6.7944e-09, 1.0078e-05, 3.2414e-06, 1.9170e-09, 8.1016e-09, 7.3586e-06,\n",
            "        7.5505e-09, 1.7744e-07, 2.1656e-06, 9.7844e-07, 2.5155e-06, 6.1413e-06,\n",
            "        3.6299e-09, 9.8557e-08, 7.4840e-07, 1.3501e-06, 5.0292e-07, 1.3102e-06,\n",
            "        1.6278e-08, 9.5211e-08, 2.0537e-10, 1.2425e-07, 1.5842e-05, 8.0332e-08,\n",
            "        8.7465e-06, 3.7789e-05, 1.1863e-08, 4.8629e-07, 9.0631e-07, 3.5959e-08,\n",
            "        1.2416e-05, 8.3826e-11, 8.3202e-07, 6.1928e-07, 2.0597e-08, 4.1268e-07,\n",
            "        9.8453e-08, 2.0374e-08, 4.9756e-07, 6.8906e-11, 6.9663e-08, 6.5686e-09,\n",
            "        2.0774e-09, 5.9625e-09, 2.4515e-06, 1.9840e-07, 2.1154e-06, 3.9293e-06,\n",
            "        2.7292e-05, 2.5703e-08, 3.8088e-05, 5.6266e-07, 1.5807e-04, 1.1071e-07,\n",
            "        1.9459e-07, 2.4168e-04, 8.1457e-10, 1.1865e-05, 5.4165e-07, 1.6656e-06,\n",
            "        1.8817e-08, 9.4860e-06, 3.8080e-07, 4.7177e-10, 6.1608e-09, 4.9533e-05,\n",
            "        2.8446e-06, 1.1775e-07, 7.5770e-09, 3.0264e-06, 7.3826e-11, 1.8651e-05,\n",
            "        3.4511e-07, 6.9337e-08, 4.1578e-06, 1.2081e-06, 3.1436e-07, 5.7399e-08,\n",
            "        6.0652e-07, 1.9234e-05, 1.0697e-10, 5.5740e-07, 2.4533e-07, 1.8357e-07,\n",
            "        5.9044e-09, 4.6006e-05, 8.5163e-10, 1.7220e-08, 4.0817e-06, 2.9260e-07,\n",
            "        1.5003e-08, 1.7932e-07, 2.9482e-05, 1.7483e-09, 1.8348e-08, 1.6800e-08,\n",
            "        3.0579e-06, 7.0109e-08, 1.5910e-06, 1.3890e-07, 2.7712e-09, 5.0192e-08,\n",
            "        7.3439e-06, 9.8118e-06, 1.3045e-06, 3.2033e-07, 6.0268e-07, 2.0848e-06,\n",
            "        3.9862e-07, 9.6821e-08, 2.5923e-07, 2.3832e-08, 2.0100e-06, 4.6586e-09,\n",
            "        2.1182e-08, 5.6527e-08, 6.7271e-05, 2.5944e-08, 1.5451e-09, 1.0660e-07,\n",
            "        2.8455e-06, 3.2581e-07, 2.4950e-07, 2.1870e-06, 3.6822e-08, 3.4622e-07,\n",
            "        1.9891e-08, 4.9396e-07, 6.4748e-09, 9.7120e-07, 4.0627e-07, 1.2556e-06,\n",
            "        4.2193e-09, 1.1677e-06, 5.3907e-10, 4.8741e-09, 1.9334e-07, 4.8699e-08,\n",
            "        2.1810e-08, 2.1488e-07, 1.0343e-07, 4.4982e-08, 1.5445e-08, 8.5763e-06,\n",
            "        2.1906e-06, 3.1124e-09, 6.4236e-08, 2.1339e-07, 9.1642e-06, 1.6985e-06,\n",
            "        2.8442e-08, 8.4634e-08, 1.0693e-07, 8.5507e-07, 8.7285e-06, 1.1031e-08,\n",
            "        4.5346e-06, 1.4780e-08, 1.2248e-06, 1.0578e-08, 2.9183e-07, 6.7560e-08,\n",
            "        1.9103e-08, 1.5229e-08, 8.9461e-06, 7.7166e-10, 2.7918e-08, 1.8861e-08,\n",
            "        1.6040e-07, 9.3007e-06, 5.3013e-06, 6.7094e-07, 3.5861e-09, 1.4226e-06,\n",
            "        4.6333e-07, 5.7981e-05, 1.0571e-05, 9.6898e-06, 8.1983e-09, 6.0276e-08,\n",
            "        1.3888e-05, 1.0018e-07, 4.4649e-07, 7.9490e-08, 4.8255e-07, 6.6243e-08,\n",
            "        1.2246e-09, 3.1747e-07, 1.3452e-04, 7.5885e-07, 1.6320e-06, 8.2338e-06,\n",
            "        1.5561e-04, 2.5295e-06, 3.5585e-06, 8.4950e-08, 1.7252e-03, 2.2772e-07,\n",
            "        2.2045e-09, 6.8173e-08, 5.1115e-05, 1.1150e-07, 7.3685e-06, 5.7225e-09,\n",
            "        1.0763e-05, 4.0513e-07, 1.2908e-05, 5.0479e-08, 2.8909e-07, 4.2371e-06,\n",
            "        5.6235e-09, 1.3704e-07, 2.5272e-08, 2.4863e-06, 2.0875e-08, 4.2981e-05,\n",
            "        3.4476e-08, 7.5746e-07, 4.9985e-08, 2.1081e-07, 6.2071e-10, 2.5999e-09,\n",
            "        1.4585e-07, 2.0236e-07, 1.3759e-06, 2.3449e-06, 1.5918e-07, 2.0659e-06,\n",
            "        4.2598e-06, 5.4522e-08, 1.6554e-06, 3.4088e-08, 2.7909e-06, 1.6147e-08,\n",
            "        5.8212e-08, 3.6591e-09, 1.3942e-09, 1.9835e-08, 9.2758e-05, 2.7735e-08,\n",
            "        4.0540e-06, 3.2183e-06, 1.7989e-07, 5.2375e-05, 1.8784e-07, 4.5588e-08,\n",
            "        1.0164e-05, 2.8404e-06, 9.7413e-07, 1.0734e-07, 2.1802e-04, 2.6788e-07,\n",
            "        4.1232e-09, 7.9900e-06, 1.7230e-07, 1.6715e-08, 2.8230e-06, 6.5620e-09,\n",
            "        4.2597e-06, 1.5178e-07, 2.7750e-07, 4.8625e-08, 5.2180e-09, 9.2289e-06,\n",
            "        2.9778e-07, 1.8874e-08, 8.1256e-07, 6.1052e-08, 6.3471e-07, 1.6135e-09,\n",
            "        8.1505e-08, 5.4500e-07, 9.4252e-09, 8.7030e-08, 6.1448e-11, 4.2847e-08,\n",
            "        1.4541e-06, 1.9526e-09, 3.3025e-07, 6.8482e-05, 3.0913e-09, 3.7698e-07,\n",
            "        5.1625e-05, 7.7607e-08, 2.3228e-09, 7.1550e-07, 5.1987e-06, 1.5534e-05,\n",
            "        8.5565e-07, 2.6895e-05, 1.6598e-08, 2.2037e-09, 1.6389e-08, 6.9958e-08,\n",
            "        3.6118e-10, 9.7420e-07, 4.4658e-06, 6.0544e-07, 4.6029e-05, 1.5868e-07,\n",
            "        1.2968e-06, 1.4173e-07, 2.0117e-08, 1.7117e-08, 1.6630e-06, 3.4236e-08,\n",
            "        1.7726e-07, 6.5828e-06, 1.3286e-07, 6.9876e-07, 1.8320e-07, 7.3678e-09,\n",
            "        4.3569e-06, 3.5921e-07, 3.6569e-08, 5.2363e-09, 1.2097e-05, 1.6908e-06,\n",
            "        9.2936e-07, 5.9985e-08, 8.4782e-08, 2.0915e-08, 3.5485e-10, 5.4188e-08,\n",
            "        7.3304e-08, 1.0768e-06, 4.8132e-06, 3.3194e-06, 2.7521e-07, 4.3090e-07,\n",
            "        1.8294e-07, 4.5026e-08, 2.7913e-06, 3.6654e-06, 1.6838e-07, 1.6673e-07,\n",
            "        2.6118e-11, 1.3090e-11, 4.9475e-05, 6.2246e-08, 1.0904e-07, 4.5779e-06,\n",
            "        1.0862e-06, 3.2232e-06, 1.9068e-06, 2.5632e-09, 1.2808e-07, 3.3752e-08,\n",
            "        2.6269e-05, 1.5632e-08, 1.9087e-06, 1.1024e-06, 2.1766e-06, 4.3118e-05,\n",
            "        1.8807e-07, 5.0179e-05, 3.1083e-08, 2.0340e-08, 5.8801e-07, 5.2501e-06,\n",
            "        2.2481e-07, 1.2274e-08, 2.8667e-06, 1.1624e-06, 1.5634e-08, 2.5714e-07,\n",
            "        1.7147e-07, 2.8353e-10, 1.6427e-07, 8.1641e-07, 8.5458e-08, 3.9727e-07,\n",
            "        4.9418e-09, 1.6532e-05, 3.8767e-08, 5.4515e-07, 6.9151e-06, 5.3801e-06,\n",
            "        2.0205e-10, 3.5985e-06, 5.0047e-06, 7.6707e-09, 4.3596e-08, 8.0305e-08,\n",
            "        1.0674e-07, 1.6083e-06, 1.7526e-06, 8.6704e-08, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "emb = Embedding()\n",
        "emb = emb.to(device)\n",
        "input_ids = input_ids.to(device)\n",
        "segment_ids = segment_ids.to(device)\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "\n",
        "print(embeds.shape)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "attenM = attenM.to(device)\n",
        "\n",
        "SDPA= ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
        "\n",
        "S, C, A = SDPA\n",
        "\n",
        "S = S.to(device)\n",
        "C = C.to(device)\n",
        "A = A.to(device)\n",
        "\n",
        "\n",
        "print('Masks:', attenM[0][0])\n",
        "print()\n",
        "print('Scores:', S[0][0],'\\n\\nAttention M:', A[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZdJUGfKFHVU3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        Q = Q.to(device)\n",
        "        K = K.to(device)\n",
        "        V = V.to(device)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "\n",
        "        _, context, attn = ScaledDotProductAttention().to(device)(q_s, k_s, v_s, attn_mask)\n",
        "        # _, context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        attn = attn.to(device)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
        "        context = context.to(device)\n",
        "        output = nn.Linear(n_heads * d_v, d_model).to(device)(context)\n",
        "        # output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model).to(device)(output + residual), attn\n",
        "        # return nn.LayerNorm(d_model)(output + residual), attn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM7RyTwaHVU3",
        "outputId": "15bb8ad0-eeca-4b09-b68e-d3c6eb1d7930"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0019, 0.0020, 0.0023,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0016, 0.0021, 0.0017,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0016, 0.0021, 0.0014,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        ...,\n",
              "        [0.0020, 0.0019, 0.0027,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0015, 0.0019, 0.0022,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0016, 0.0020, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],\n",
              "       device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb = Embedding()\n",
        "emb = emb.to(device)\n",
        "input_ids = input_ids.to(device)\n",
        "segment_ids = segment_ids.to(device)\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "embeds = embeds.to(device)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "attenM = attenM.to(device)\n",
        "\n",
        "MHA= MultiHeadAttention().to(device)(embeds, embeds, embeds, attenM)\n",
        "# MHA= MultiHeadAttention()(embeds, embeds, embeds, attenM)\n",
        "\n",
        "Output, A = MHA\n",
        "Output = Output.to(device)\n",
        "A = A.to(device)\n",
        "\n",
        "A[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DDNwNFbfHVU3"
      },
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(gelu(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xmvDFKAaHVU3"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
        "        attn = attn.to(device)\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)\n",
        "        enc_outputs = enc_outputs.to(device)\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ru8Kl0OaHVU3"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.embedding = self.embedding.to(device)\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ1 = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        embed_weight.to(device)\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        input_ids = input_ids.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        masked_pos = masked_pos.to(device)\n",
        "\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids).to(device)\n",
        "        # enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0]))\n",
        "        logits_clsf = self.classifier(h_pooled)\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
        "\n",
        "        h_masked = torch.gather(output, 1, masked_pos)\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
        "\n",
        "        return logits_lm, logits_clsf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPyJk_LAHVU3",
        "outputId": "f88cb4fc-a321-44b1-86a1-3443f4cd6a0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0010 loss = 19.513000\n",
            "Epoch: 0020 loss = 16.196199\n",
            "Epoch: 0030 loss = 14.250731\n",
            "Epoch: 0040 loss = 13.268578\n",
            "Epoch: 0050 loss = 12.443357\n",
            "Epoch: 0060 loss = 11.979315\n"
          ]
        }
      ],
      "source": [
        "model = BERT()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "input_ids = torch.LongTensor(arrange_tokens([nums[0] for nums in batch]))\n",
        "segment_ids = torch.LongTensor(arrange_tokens([nums[1] for nums in batch]))\n",
        "masked_tokens = torch.LongTensor(arrange_tokens([nums[2] for nums in batch]))\n",
        "masked_pos = torch.LongTensor(arrange_tokens([nums[3] for nums in batch]))\n",
        "isNext = torch.LongTensor([nums[4] for nums in batch])\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "segment_ids = segment_ids.to(device)\n",
        "masked_tokens = masked_tokens.to(device)\n",
        "masked_pos = masked_pos.to(device)\n",
        "isNext = isNext.to(device)\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_clsf = model(input_ids.to(device), segment_ids.to(device), masked_pos.to(device))\n",
        "    # logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    loss_clsf = criterion(logits_clsf, isNext)\n",
        "    loss = loss_lm + loss_clsf\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru_7WkFGHVU4"
      },
      "outputs": [],
      "source": [
        "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
        "# print(text)\n",
        "# print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
        "\n",
        "# input_ids = input_ids.to(device)\n",
        "# segment_ids = segment_ids.to(device)\n",
        "# masked_tokens = masked_tokens.to(device)\n",
        "# masked_pos = masked_pos.to(device)\n",
        "# isNext = isNext.to(device)\n",
        "\n",
        "# logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "# logits_lm = logits_lm.data.max(2)[1][0].data.cpu().numpy()\n",
        "# print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "# print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "# logits_clsf = logits_clsf.data.max(1)[1].data.cpu().numpy()[0]\n",
        "# print('isNext : ', True if isNext else False)\n",
        "# print('predict isNext : ',True if logits_clsf else False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
